{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Link/edge prediction demo for cora dataset (homegeneous network) where all nodes are papers and edges between nodes are citation links, e.g., paper A cites paper B. \n",
    "\n",
    "Each paper has a **subject** attribute with one of 7 values denoting the subject area of the paper.\n",
    "\n",
    "This demo notebook **demonstrates how to predict citation links/edges between papers** using the random walk-based representation learning method Node2Vec.\n",
    "\n",
    "**References:** \n",
    "\n",
    "**[1]** Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import isclose\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stellargraph.data import EdgeSplitter\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default parameters for Node2Vec\n",
    "parameters = {\n",
    "    \"p\": 1.,  # Parameter p\n",
    "    \"q\": 1.,  # Parameter q\n",
    "    \"dimensions\": 128,  # dimensionality of node2vec embeddings\n",
    "    \"num_walks\": 10,  # Number of walks from each node\n",
    "    \"walk_length\": 80,  # Walk length\n",
    "    \"window_size\": 10,  # Context size for word2vec\n",
    "    \"iter\": 1,  # number of SGD iterations (epochs)\n",
    "    \"workers\": multiprocessing.cpu_count(),  # number of workers for word2vec\n",
    "    \"weighted\": False,  # is graph weighted?\n",
    "    \"directed\": False,  # are edges directed?\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "The dataset is the citation network Cora.\n",
    "\n",
    "It can be downloaded by clicking [here](https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz)\n",
    "\n",
    "The following is the description of the dataset from the publisher,\n",
    "\n",
    "> The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words. The README file in the dataset provides more details. \n",
    "\n",
    "For this demo, we ignore the word vector associated with each paper. We are only interested in the network structure.\n",
    "\n",
    "Download and unzip the cora.tgz file to a location on your computer. For the purposes of this demo, we are only interested in the **cora.cites** file that is a representation of the graph as an edge list.\n",
    "\n",
    "We assume that the dataset is stored in the directory\n",
    "\n",
    "`../data/cora/cora.cites`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"../data/cora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_location = os.path.expanduser(os.path.join(data_dir, \"cora.cites\"))\n",
    "g_nx = nx.read_edgelist(path=cora_location)\n",
    "\n",
    "# load the node attribute data\n",
    "cora_data_location = os.path.expanduser(os.path.join(data_dir, \"cora.content\"))\n",
    "node_attr = pd.read_csv(cora_data_location, sep='\\t', header=None)\n",
    "values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows()}\n",
    "nx.set_node_attributes(g_nx, values, 'subject')\n",
    "\n",
    "# Select the largest connected component. For clarity we ignore isolated\n",
    "# nodes and subgraphs; having these in the data does not prevent the\n",
    "# algorithm from running and producing valid results.\n",
    "g_nx_ccs = (g_nx.subgraph(c).copy() for c in nx.connected_components(g_nx))\n",
    "g_nx = max(g_nx_ccs, key=len)\n",
    "print(\"Largest subgraph statistics: {} nodes, {} edges\".format(\n",
    "    g_nx.number_of_nodes(), g_nx.number_of_edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct train and test splits of the input data\n",
    "\n",
    "We are going to tackle link prediction as a supervised learning problem whereas we are going to train a binary classifier to predict a link, or not, between to nodes in the graph. \n",
    "\n",
    "For supervised classification, we need a training set of positive and negative examples of links and a corresponding **train** graph with the positive links removed; this graph is used for representation learning.\n",
    "\n",
    "For evaluation, we will use a hold out dataset, that is a set of positive and negative examples of links and a corresponding **test** graph.\n",
    "\n",
    "We are going to split our input graph into a **train** and **test** graphs using the EdgeSplitter class in stellar.data.edge_splitter. We are going to use the **train** graph for training the predictive model (a binary classifier that given two nodes it will predict whether a link between these two nodes should exist or not) and the **test** graph for evaluating the predictive model's performance on hold out data. \n",
    "\n",
    "Each graph will have the same number of nodes as the input graph but the number of edges will differ as some of the edges will be removed during each split and used as the positive samples for training the link prediction classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test graph and edge test data\n",
    "edge_splitter_test = EdgeSplitter(g_nx)\n",
    "g_test, edge_data_ids_test, edge_data_labels_test = edge_splitter_test.train_test_split( \n",
    "    p=0.15, method='global'\n",
    ")\n",
    "\n",
    "# Train graph and edge train data\n",
    "edge_splitter_val = EdgeSplitter(g_test, g_nx)\n",
    "g_val, edge_data_ids_val, edge_data_labels_val = edge_splitter_val.train_test_split(\n",
    "    p=0.15, method='global'\n",
    ")\n",
    "\n",
    "# Train graph and edge train data\n",
    "edge_splitter_train = EdgeSplitter(g_val, g_nx)\n",
    "g_train, edge_data_ids_train, edge_data_labels_train = edge_splitter_train.train_test_split(\n",
    "    p=0.1, method='global'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert graphs\n",
    "\n",
    "The node2vec reference implementation requires that the given graph is undirected and homogeneous. We cast the **train** and **test** graphs accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train and Test graphs should be of type nx.Graph\n",
    "g_test = nx.Graph(g_test)\n",
    "g_val = nx.Graph(g_val)\n",
    "g_train = nx.Graph(g_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the link prediction model and evaluate on test data\n",
    "\n",
    "We are going to train a link prediction classifier in three steps. \n",
    "1. We use Node2Vec, [1], to calculate node embeddings. These embeddings are learned in such a way to ensure that nodes that are close in the graph remain close in the embedding space.\n",
    "2. We calculate link/edge embeddings for the positive and negative edge samples by applying a binary operator on the embeddings of the source and target nodes of each sampled edge. We consider 4 different operators, hadamard, l1, l2, and average; the paper in [1] provides a detailed description of these operators. All operators produce link embeddings that have equal dimensionality to the input node embeddings (128 dimensions for our example.) \n",
    "3. Given the embeddings of the positive and negative examples, we train a logistic regression classifier to predict a binary value indicating whether an edge between two nodes should exist or not.\n",
    "4. We evaluate the performance of the link classifier for each of the 4 operators on the training data and select the best performing one. The selected operator is used to predict the hold out, test, data with node embeddings calculated on the **test** graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_walk_length = 100\n",
    "num_walks_per_node = 10\n",
    "p = 1.\n",
    "q = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(StellarGraph(g_train))\n",
    "\n",
    "walks = rw.run(nodes=list(g_train.nodes()), # root nodes\n",
    "               length=random_walk_length,  # maximum length of a random walk\n",
    "               n=num_walks_per_node,        # number of random walks per root node \n",
    "               p=p,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "               q=q        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "              )\n",
    "print(\"Number of random walks: {}\".format(len(walks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128 \n",
    "context_window_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(walks, size=embedding_dim, window=context_window_size, min_count=0, sg=1, workers=2, iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_data_ids_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def operator_hadamard(u, v):\n",
    "    return u*v\n",
    "\n",
    "def operator_avg(u, v):\n",
    "    return (u+v)/2.0\n",
    "\n",
    "def operator_l2(u, v):\n",
    "    return (u-v)**2\n",
    "\n",
    "def operator_l1(u, v):\n",
    "    return np.abs(u-v)\n",
    "\n",
    "def transform(model, edge_data, binary_operator):\n",
    "    \"\"\"\n",
    "    It calculates edge features for the given binary operator applied to the node features in data_edge\n",
    "    :param edge_data: (2-tuple) It is a list of pairs of nodes that make an edge in the graph\n",
    "    :param binary_operator: The binary operator to apply to the node features to calculate an edge feature\n",
    "    :return: Features in X (Nxd array where N is the number of edges and d is the dimensionality of the edge\n",
    "        features that is the same as the dimensionality of the node features) and edge labels in y (0 for no edge\n",
    "        and 1 for edge).    \n",
    "    \"\"\"\n",
    "    X = []  # data matrix, each row is a d-dimensional feature of an edge\n",
    "        \n",
    "    for ids in edge_data:        \n",
    "        X.append(binary_operator(model[ids[0]], model[ids[1]]))\n",
    "  \n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_train_features = transform(model=model, \n",
    "                                edge_data=edge_data_ids_train, \n",
    "                                binary_operator=operator_l2)\n",
    "edge_val_features = transform(model=model, \n",
    "                              edge_data=edge_data_ids_val, \n",
    "                              binary_operator=operator_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_train_features.shape, edge_val_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise representations of link embeddings\n",
    "\n",
    "Learned link embeddings have 128 dimensions but for visualisation we project them down to 2 dimensions using the PCA algorithm ([link](https://en.wikipedia.org/wiki/Principal_component_analysis)). \n",
    "\n",
    "Green points represent positive edges and red points represent negative (no edge should exist between the corresponding vertices) edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate edge features for validation data\n",
    "edge_data = (edge_data_ids_val, edge_data_labels_val)\n",
    "X, y = edge_val_features, edge_data_labels_val\n",
    "\n",
    "# Learn a projection from 128 dimensions to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# plot the 2-dimensional points\n",
    "red = y == 0\n",
    "green = y == 1\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.scatter(X_transformed[red, 0], X_transformed[red, 1], c=\"r\", alpha=0.8)\n",
    "plt.scatter(X_transformed[green, 0], X_transformed[green, 1], c=\"g\", alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "We can use the edge features to train a classifier. We can then use this classifier to predict the test set and measure its generalisation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegressionCV( Cs=10, cv=5, scoring='roc_auc', max_iter=1000, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(edge_train_features, edge_data_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_predictions = clf.predict_proba(edge_train_features)\n",
    "if clf.classes_[0]==1:\n",
    "    train_predictions = train_predictions[:, 0]\n",
    "else:\n",
    "    train_predictions = train_predictions[: ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(edge_data_labels_train, train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predictions = clf.predict_proba(edge_val_features)\n",
    "if clf.classes_[0]==1:\n",
    "    val_predictions = val_predictions[:, 0]\n",
    "else:\n",
    "    val_predictions = val_predictions[: ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(edge_data_labels_val, val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation learning on the validation graph\n",
    "\n",
    "We are going to repeat the representation learning step on the validation graph because it differs from the train graph.\n",
    "\n",
    "**Note:** We are not going to retrain the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rw = BiasedRandomWalk(StellarGraph(g_val))\n",
    "\n",
    "walks = rw.run(nodes=list(g_val.nodes()), # root nodes\n",
    "               length=random_walk_length,  # maximum length of a random walk\n",
    "               n=num_walks_per_node,        # number of random walks per root node \n",
    "               p=p,       # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "               q=q        # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "              )\n",
    "print(\"Number of random walks: {}\".format(len(walks)))\n",
    "\n",
    "model = Word2Vec(walks, size=embedding_dim, window=context_window_size, min_count=0, sg=1, workers=2, iter=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edge_test_features = transform(model=model, \n",
    "                              edge_data=edge_data_ids_test, \n",
    "                              binary_operator=operator_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = clf.predict_proba(edge_test_features)\n",
    "if clf.classes_[0]==1:\n",
    "    test_predictions = test_predictions[:, 0]\n",
    "else:\n",
    "    test_predictions = test_predictions[: ,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(edge_data_labels_test, test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practical-ml",
   "language": "python",
   "name": "practical-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
